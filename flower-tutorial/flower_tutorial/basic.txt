# Federated Learning with Flower
In federated learning, the server sends global model parameters to the client, and the client updates the local model with parameters received from the server. It then trains the model on the local data (which changes the model parameters locally) and sends the updated/changed model parameters back to the server (or, alternatively, it sends just the gradients back to the server, not the full model parameters).

# Constructing Messages
In Flower, the server and clients communicate by sending and receiving Message objects. A Message carries a RecordDict as its main payload. The RecordDict is like a Python dictionary that can contain multiple records of different types. There are three main types of records:

ArrayRecord: Contains model parameters as a dictionary of NumPy arrays

MetricRecord: Contains training or evaluation metrics as a dictionary of integers, floats, lists of integers, or lists of floats.

ConfigRecord: Contains configuration parameters as a dictionary of integers, floats, strings, booleans, or bytes. Lists of these types are also supported.



from flwr.app import ArrayRecord, MetricRecord, ConfigRecord, RecordDict

# ConfigRecord can be used to communicate configs between ServerApp and ClientApp
# They can hold scalars, but also strings and booleans
config = ConfigRecord(
    {"batch_size": 32, "use_augmentation": True, "data-path": "/my/dataset"}
)

# MetricRecords expect scalar-based metrics (i.e. int/float/list[int]/list[float])
# By limiting the types Flower can aggregate MetricRecords automatically
metrics = MetricRecord({"accuracy": 0.9, "losses": [0.1, 0.001], "perplexity": 2.31})

# ArrayRecord objects are designed to communicate arrays/tensors/weights from ML models
array_record = ArrayRecord(my_model.state_dict())  # for a PyTorch model
array_record_other = ArrayRecord(my_model.to_numpy_ndarrays())  # for other ML models

# A RecordDict is like a dictionary that holds named records.
# This is the main payload of a Message
rd = RecordDict({"my-config": config, "metrics": metrics, "my-model": array_record})




num-server-rounds = 3
Number of federated learning rounds the server will execute. Each round = one full cycle of client selection -> local training -> aggregation

fraction-train = 0.5
Fraction of clients participating in each round of training. If you have 10 clients, 0.5 -> 5 clients selected per round.

local-epochs = 1
Number of local epochs each selected client trains on its local data per round

lr = 0.01
Learning rate for the local optimizer used by each client

ðŸŒ What is a â€œPartitionâ€?
the term "partition" in federated learning(and in Flower framework) refers to how the dataset is divided(partitioned) among multiple clients. This concept is fundamental because data in not centralized in FL, each client owns its own portion of data (called a "partition")

In federated learning, a partition = one client's local dataset.
if you have a dataset like MNIST(60,000 samples) and 10 clients, you might partition it into 10 sunsets, each containing 6,000 samples.
    Each subset is a partition 
    Each client only sees and trains its own partition

So instead of: centralized => One dataset shared bt automatically
federated setup: Client 1 => Data partition #1
                 Client 2 => Data partition #2
                 ...
                 Client N => Data partition #N

ðŸ§© Why Partitioning Matters

Partitioning defines how heterogeneous or diverse your clientsâ€™ data are.
There are two main styles:
    1.IID(Independent and Identically Distributed): Each client gets a random sample of the overall dataset(same distribution)
    example: Each client has digits 0-9 equally in MNIST

    2. Non-IID: Clients get different distributions, simulates real-world scenarios
    example: Client 1 has mostly digit 0-2, Client 2 has 3-5, etc
ðŸ‘‰ Most real-world FL systems are Non-IID, because usersâ€™ data distributions differ.



# Define the Flower ClientApp
Federated learning systems consist of a server and multiple nodes or clients. In Flower, we create a ServerApp and a ClientApp to run the server-side and client-side code, respectively.

The core functionality of the ClientApp is to perform some action with the local data that the node it runs from (e.g. an edge device, a server in a data center, or a laptop) has access to. In this tutorial such action is to train and evaluate the small CNN model defined earlier using the local training and validation data.

# Training
We can define how the ClientApp performs training by wrapping a function with the @app.train() decorator. In this case we name this function train because weâ€™ll use it to train the model on the local data. The function always expects two arguments:
A Message: The message received from the server. It contains the model parameters and any other configuration information sent by the server.

A Context: The context object that contains information about the node executing the ClientApp and about the current run.

Through the context you can retrieve the config settings defined in the pyproject.toml of your app. The context can be used to persist the state of the client across multiple calls to train or evaluate. In Flower, ClientApps are ephemeral objects that get instantiated for the execution of one Message and destroyed when a reply is communicated back to the server.